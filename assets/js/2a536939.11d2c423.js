"use strict";(self.webpackChunkopen_assistant=self.webpackChunkopen_assistant||[]).push([[7991],{3905:(e,a,t)=>{t.d(a,{Zo:()=>h,kt:()=>m});var r=t(67294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var s=r.createContext({}),p=function(e){var a=r.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},h=function(e){var a=p(e.components);return r.createElement(s.Provider,{value:a},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},u=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),d=p(t),u=n,m=d["".concat(s,".").concat(u)]||d[u]||c[u]||i;return t?r.createElement(m,o(o({ref:a},h),{},{components:t})):r.createElement(m,o({ref:a},h))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,o=new Array(i);o[0]=u;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l[d]="string"==typeof e?e:n,o[1]=l;for(var p=2;p<i;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},26193:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=t(87462),n=(t(67294),t(3905));const i={},o="Retrieval Directions and Research Papers",l={unversionedId:"research/retrieval",id:"research/retrieval",title:"Retrieval Directions and Research Papers",description:"Dataset and Benchmark",source:"@site/docs/research/retrieval.md",sourceDirName:"research",slug:"/research/retrieval",permalink:"/Open-Assistant/docs/research/retrieval",draft:!1,tags:[],version:"current",frontMatter:{}},s={},p=[{value:"Dataset and Benchmark",id:"dataset-and-benchmark",level:2},{value:"Search Algorithm",id:"search-algorithm",level:2},{value:"Links",id:"links",level:3},{value:"Relevant Papers",id:"relevant-papers",level:3},{value:"1. Retrieval-Index",id:"1-retrieval-index",level:2},{value:"Links",id:"links-1",level:3},{value:"Relevant Papers",id:"relevant-papers-1",level:3},{value:"2. Plugin-based approach",id:"2-plugin-based-approach",level:2},{value:"Relevant Papers",id:"relevant-papers-2",level:3},{value:"3. Embedding-based approach",id:"3-embedding-based-approach",level:2},{value:"3a",id:"3a",level:3},{value:"3b",id:"3b",level:3},{value:"3c",id:"3c",level:3},{value:"Relevant papers",id:"relevant-papers-3",level:3},{value:"Paper summaries",id:"paper-summaries",level:2},{value:"Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - &quot;RETRO&quot;",id:"borgeaud-et-al-2020-improving-language-models-by-retrieving-from-trillions-of-tokens---retro",level:3},{value:"Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input",id:"bertsch-et-al-2023-unlimiformer-long-range-transformers-with-unlimited-length-input",level:3},{value:"Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - &quot;Contriver&quot;",id:"izacard-et-al-2022-unsupervised-dense-information-retrieval-with-contrastive-learning---contriver",level:3},{value:"Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools",id:"schick-et-al-2023-toolformer-language-models-can-teach-themselves-to-use-tools",level:3},{value:"Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training",id:"guu-et-al-2020-realm-retrieval-augmented-language-model-pre-training",level:3},{value:"Zamani et al. 2022: Retrieval-Enhanced Machine Learning",id:"zamani-et-al-2022-retrieval-enhanced-machine-learning",level:3},{value:"Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models",id:"thakur-et-al-2021-beir-a-heterogeneous-benchmark-for-zero-shot-evaluation-of-information-retrieval-models",level:3},{value:"Other interesting papers",id:"other-interesting-papers",level:2}],h={toc:p},d="wrapper";function c(e){let{components:a,...t}=e;return(0,n.kt)(d,(0,r.Z)({},h,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"retrieval-directions-and-research-papers"},"Retrieval Directions and Research Papers"),(0,n.kt)("h2",{id:"dataset-and-benchmark"},"Dataset and Benchmark"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"BEIR\n",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.08663v4"},"https://arxiv.org/abs/2104.08663v4")," -\nBenchmark for Information Retrieval"),(0,n.kt)("li",{parentName:"ul"},"MS MARCO(part of BEIR)\n",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1611.09268v3"},"https://arxiv.org/abs/1611.09268v3")," -\nMachine Reading Comprehension Dataset / Retrieval Benchmark")),(0,n.kt)("h2",{id:"search-algorithm"},"Search Algorithm"),(0,n.kt)("h3",{id:"links"},"Links"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"ElasticSearch:\n",(0,n.kt)("a",{parentName:"li",href:"https://www.elastic.co/elasticsearch"},"https://www.elastic.co/elasticsearch")),(0,n.kt)("li",{parentName:"ul"},"Apache Lucene: ",(0,n.kt)("a",{parentName:"li",href:"https://lucene.apache.org/"},"https://lucene.apache.org/")),(0,n.kt)("li",{parentName:"ul"},"Meta Faiss:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/faiss"},"https://github.com/facebookresearch/faiss")),(0,n.kt)("li",{parentName:"ul"},"Google Scann:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/google-research/google-research/tree/master/scann"},"https://github.com/google-research/google-research/tree/master/scann")),(0,n.kt)("li",{parentName:"ul"},"Qdrant Vector DB:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/qdrant/qdrant"},"https://github.com/qdrant/qdrant")),(0,n.kt)("li",{parentName:"ul"},"Milvus Vector DB: ",(0,n.kt)("a",{parentName:"li",href:"https://milvus.io/"},"https://milvus.io/")),(0,n.kt)("li",{parentName:"ul"},"Open Retrieval Index Code:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/kenhktsui/open-information-retrieval"},"https://github.com/kenhktsui/open-information-retrieval"))),(0,n.kt)("h3",{id:"relevant-papers"},"Relevant Papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"FAISS: ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1702.08734"},"https://arxiv.org/abs/1702.08734")," -\nvector index by Facebook"),(0,n.kt)("li",{parentName:"ul"},"SCaNN: ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1908.10396"},"https://arxiv.org/abs/1908.10396")," -\nvector index by Google")),(0,n.kt)("h2",{id:"1-retrieval-index"},"1. Retrieval-Index"),(0,n.kt)("p",null,"At first, either a rule-based search or sparse vector search (e.g. BM25) or a\ndense vector search (semantic search) (e.g. BERT, Contriever) could be used. In\npractice, retrieval is a layered approach, where the first search is optimised\nfor recall and reranking is optimised for precision."),(0,n.kt)("p",null,"The first search in general is a sparse vector search, or dense vector search\n(bi-encoder). The advantage is that it is fast because document can be\npre-indexed and stored in a DB. Cosine similarity is used to find the most\nsimilar pre-indexed document embedding given the query embedding. Reranking is a\ntechnique to boost the performance of top K documents from the first search. For\nexample, cross-encoder which takes both query and document into a language\nmodel, and output a scalar relevance between 0 and 1. It has more superior\nperformance than bi-encoder because it allows interaction of query and document\nin the language model. But it is slow because no index can be pre-computed."),(0,n.kt)("h3",{id:"links-1"},"Links"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"LangChain:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/hwchase17/langchain"},"https://github.com/hwchase17/langchain")," -\nPlugins around any language model"),(0,n.kt)("li",{parentName:"ul"},"LlamaIndex:\n",(0,n.kt)("a",{parentName:"li",href:"https://github.com/jerryjliu/llama_index"},"https://github.com/jerryjliu/llama_index")," -\nGeneral Retrieval System for LMs and external data"),(0,n.kt)("li",{parentName:"ul"},"LlamaHub: ",(0,n.kt)("a",{parentName:"li",href:"https://llamahub.ai/"},"https://llamahub.ai/")," - Data Source Plugins\nfor LlamaIndex")),(0,n.kt)("h3",{id:"relevant-papers-1"},"Relevant Papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"SBERT ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1908.10084"},"https://arxiv.org/abs/1908.10084")),(0,n.kt)("li",{parentName:"ul"},"BM25+CE\n",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.08663v4"},"https://arxiv.org/abs/2104.08663v4")),(0,n.kt)("li",{parentName:"ul"},"RALM ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2302.00083"},"https://arxiv.org/abs/2302.00083")),(0,n.kt)("li",{parentName:"ul"},"ColBert ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.12832"},"https://arxiv.org/abs/2004.12832")),(0,n.kt)("li",{parentName:"ul"},"DPR ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.04906"},"https://arxiv.org/abs/2004.04906")),(0,n.kt)("li",{parentName:"ul"},"UPR ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2204.07496"},"https://arxiv.org/abs/2204.07496")),(0,n.kt)("li",{parentName:"ul"},"...")),(0,n.kt)("h2",{id:"2-plugin-based-approach"},"2. Plugin-based approach"),(0,n.kt)("p",null,"In this approach, retrieval as a tool, is embedded into the training data,\nincluding:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"when a retrieval is required"),(0,n.kt)("li",{parentName:"ul"},"how to do a search (what to search)"),(0,n.kt)("li",{parentName:"ul"},"how to use search result As such, a language model trained with this data is\nable to do retrieval from a next token prediction objective.")),(0,n.kt)("h3",{id:"relevant-papers-2"},"Relevant Papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Toolformer: ",(0,n.kt)("a",{parentName:"li",href:"http://arxiv.org/abs/2302.04761"},"http://arxiv.org/abs/2302.04761")),(0,n.kt)("li",{parentName:"ul"},"...")),(0,n.kt)("h2",{id:"3-embedding-based-approach"},"3. Embedding-based approach"),(0,n.kt)("p",null,"The embedding-based approach ingests retrieved information directly into the\nmodel, e.g. via an additional encoder and cross-attention."),(0,n.kt)("h3",{id:"3a"},"3a"),(0,n.kt)("p",null,"Simply inject embeddings via cross-attention or a similar mechanism."),(0,n.kt)("h3",{id:"3b"},"3b"),(0,n.kt)("p",null,"Inject embeddings based on a more sophisticated architecture, e.g. make the\nmodel decide to do retrieval and only then inject embeddings. Might be hard to\ntrain."),(0,n.kt)("h3",{id:"3c"},"3c"),(0,n.kt)("p",null,"Train retrieval index jointly with the injection. Possibly infeasible as the\nindex needs to be re-updated during training."),(0,n.kt)("h3",{id:"relevant-papers-3"},"Relevant papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"RETRO: ",(0,n.kt)("a",{parentName:"li",href:"http://arxiv.org/abs/2112.04426"},"http://arxiv.org/abs/2112.04426")),(0,n.kt)("li",{parentName:"ul"},"REALM: ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2002.08909"},"https://arxiv.org/abs/2002.08909")),(0,n.kt)("li",{parentName:"ul"},"RAG: ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2005.11401"},"https://arxiv.org/abs/2005.11401")),(0,n.kt)("li",{parentName:"ul"},"Atlas ",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2208.03299"},"https://arxiv.org/abs/2208.03299")),(0,n.kt)("li",{parentName:"ul"},"Unilimiformer\n",(0,n.kt)("a",{parentName:"li",href:"http://arxiv.org/abs/2305.01625"},"http://arxiv.org/abs/2305.01625"))),(0,n.kt)("h2",{id:"paper-summaries"},"Paper summaries"),(0,n.kt)("h3",{id:"borgeaud-et-al-2020-improving-language-models-by-retrieving-from-trillions-of-tokens---retro"},'Borgeaud et al. 2020: Improving Language Models by Retrieving from Trillions of Tokens - "RETRO"'),(0,n.kt)("p",null,'Idea: Use BERT (Devlin et al. 2018) as a contextual encoder for chunks of size\n64 of the training data. Then train an encoder-decoder transformer model with\ninputs and similar (not too similar / same) input chunks retrieved by BERT\nembedding similarity - all done in a causal way (retrieve only "from the past").\nThe Cross-Attention is replaced by a Chunked Cross Attention optimized for\nbatches of similar retrieved chunks. They pre-filter their dataset such that\ndata duplicates cannot easily leak information via retrieval. This was scaled to\n2T tokens and a 7.5 B parameter model exceeding GPT-3 performance. RETROfitting\nof a pre-trained transformer also works, with small losses in perplexity (0.3),\nbut a lot faster training (6 % of training sequences = 6M seq \xe0 2048 tokens).\nThis is not fine-tuning but just training the cross-attention, keeping\npre-trained weights fixed. Larger models benefit from more nearest neighbors,\ni.e. the 7B can utilize 40 nearest neighbor chunks, a 172M model only 10 NNs.'),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"http://arxiv.org/abs/2112.04426"},"http://arxiv.org/abs/2112.04426")),(0,n.kt)("h3",{id:"bertsch-et-al-2023-unlimiformer-long-range-transformers-with-unlimited-length-input"},"Bertsch et al. 2023: Unlimiformer: Long-Range Transformers with Unlimited Length Input"),(0,n.kt)("p",null,'Idea: Use retrieval to actually maximize overlap of "query embeddings" with\nembeddings from an encoder (in a encoder-decoder architecture). Essentially it\nis an ideal approximation of the softmax in the Cross-Attention over all\nprevious tokens (in the encoder inputs).'),(0,n.kt)("p",null,"Code:\n",(0,n.kt)("a",{parentName:"p",href:"https://github.com/abertsch72/unlimiformer"},"https://github.com/abertsch72/unlimiformer"),"\nPaper: ",(0,n.kt)("a",{parentName:"p",href:"http://arxiv.org/abs/2305.01625"},"http://arxiv.org/abs/2305.01625")),(0,n.kt)("h3",{id:"izacard-et-al-2022-unsupervised-dense-information-retrieval-with-contrastive-learning---contriver"},'Izacard et al. 2022: Unsupervised Dense Information Retrieval with Contrastive Learning - "Contriver"'),(0,n.kt)("p",null,"They present Contriver, an open-source implementation of their novel approach to\ninformation retrieval using neural networks that outperforms traditional methods\nand can be applied to a wide range of retrieval settings. The main idea behind\nContriver is to use contrastive learning to train dense retrievers for\ninformation retrieval. Their key contribution is showing that this approach\nleads to strong performance in various retrieval settings, including\ncross-lingual retrieval, and outperforms traditional unsupervised term-frequency\nmethods such as BM25."),(0,n.kt)("p",null,"Specifically, on the BEIR benchmark, their unsupervised model outperforms BM25\non 11 out of 15 datasets for the Recall@100. When used as pre-training before\nfine-tuning, either on a few thousands in-domain examples or on the large MS\nMARCO dataset, their contrastive model leads to improvements on the BEIR\nbenchmark."),(0,n.kt)("p",null,"Pre-trained model and source code are available on Huggingface and GitHub."),(0,n.kt)("h3",{id:"schick-et-al-2023-toolformer-language-models-can-teach-themselves-to-use-tools"},"Schick et al. 2023: Toolformer: Language Models Can Teach Themselves to Use Tools"),(0,n.kt)("p",null,"They use in-context learning of GPT-3 and some handcrafted samples to annotate a\nlanguage modeling dataset with potential uses of external tools, like QA,\nwikipedia search, a calculator, machine translation and a calendar - via text\ntags for those tools and respective tool queries. They use this data then to\nfine-tune GPT-2/GPT-J models, implement according tools and train with up to 25k\nexamples per API, max sequence length 1,024. They outperform other language\nmodels with large margin when using tools and are comparable to larger ones when\nonly fine-tuned on the tool-based dataset."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"http://arxiv.org/abs/2302.04761"},"http://arxiv.org/abs/2302.04761")),(0,n.kt)("h3",{id:"guu-et-al-2020-realm-retrieval-augmented-language-model-pre-training"},"Guu et al 2020: REALM: Retrieval-Augmented Language Model Pre-Training"),(0,n.kt)("p",null,"They use retrieved information from a KB to train a MLM self-supervised and\nevaluate on QA tasks. Predecessor to RETRO."),(0,n.kt)("p",null,"The authors of the paper structure the retriever in REALM such that the\ncomputation performed for each document can be cached and asynchronously\nupdated, and selection of the best documents can be formulated as Maximum Inner\nProduct Search (MIPS). This allows for efficient retrieval of potentially\nrelevant documents from a large corpus during pre-training."),(0,n.kt)("p",null,"During pre-training, REALM backpropagates through the retrieval step that\nconsiders millions of documents, but it does not backpropagate to each\nindividual document. Instead, it uses a single encoder to encode the subset of\nretrieved samples and then backpropagates through this encoder. This approach\nallows for efficient computation during pre-training while still allowing for\neffective utilization of world knowledge."),(0,n.kt)("p",null,"(",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2002.08909)%5Bhttps://arxiv.org/abs/2002.08909%5D"},"https://arxiv.org/abs/2002.08909)[https://arxiv.org/abs/2002.08909]")),(0,n.kt)("h3",{id:"zamani-et-al-2022-retrieval-enhanced-machine-learning"},"Zamani et al. 2022: Retrieval-Enhanced Machine Learning"),(0,n.kt)("p",null,"This paper introduces a new research program called Retrieval-Enhanced Machine\nLearning (REML), which combines information retrieval techniques with machine\nlearning to improve model accuracy and interpretability. The authors describe\nthe core principles of indexing, representation, retrieval, and ranking that\nunderlie REML models, and provide examples of how these models have been applied\nin real-world scenarios."),(0,n.kt)("p",null,"The main contribution of this paper is to lay out a research agenda for REML\nthat includes several key challenges and opportunities for future work. These\ninclude developing new optimization algorithms that can handle large-scale data\nsets, exploring the use of deep learning architectures in conjunction with\nretrieval-based methods, and investigating the impact of different retrieval\nstrategies on model performance."),(0,n.kt)("p",null,"Overall, the key idea behind REML is to leverage the strengths of both\ninformation retrieval and machine learning to create more powerful and flexible\nmodels that can handle complex data sets and produce more accurate results. By\ncombining these two fields, researchers hope to pave the way for new advances in\nartificial intelligence and information access research."),(0,n.kt)("p",null,"(",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2205.01230)%5Bhttps://arxiv.org/abs/2205.01230%5D"},"https://arxiv.org/abs/2205.01230)[https://arxiv.org/abs/2205.01230]")),(0,n.kt)("h3",{id:"thakur-et-al-2021-beir-a-heterogeneous-benchmark-for-zero-shot-evaluation-of-information-retrieval-models"},"Thakur et al. 2021: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models"),(0,n.kt)("p",null,"The BEIR benchmarking tool is designed to provide a comprehensive evaluation of\ninformation retrieval models across diverse tasks and domains. It includes 18\nretrieval datasets for comparison and evaluation of model generalization,\nspanning nine different retrieval tasks such as fact checking, citation\nprediction, duplicate question retrieval, argument retrieval, news retrieval,\nquestion answering, tweet retrieval, bio-medical IR, and entity retrieval. The\nselection methodology is motivated by the need for diverse tasks and domains to\nevaluate the zero-shot capabilities of retrieval systems. The tool is\nopen-sourced with a standardized data format and easy-to-adapt code examples for\nmany different retrieval strategies."),(0,n.kt)("p",null,"They compare neural retrieval to legacy systems like BM25 and show that BM25 is\nstill a very strong baseline. The best model is a BM25 based search with\nadditional re-ranking based on a neural classifier."),(0,n.kt)("p",null,"Observations:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},'"In-domain performance is not a good indicator for out-of-domain\ngeneralization"'),(0,n.kt)("li",{parentName:"ol"},'"Term-weighting fails, document expansion captures out-of-domain keyword\nvocabulary"'),(0,n.kt)("li",{parentName:"ol"},'"Dense retrieval models with issues for out-of-distribution data"'),(0,n.kt)("li",{parentName:"ol"},'"Re-ranking and Late-Interaction models generalize well to\nout-of-distribution data"'),(0,n.kt)("li",{parentName:"ol"},'"Strong training losses for dense retrieval leads to better\nout-of-distribution performances"'),(0,n.kt)("li",{parentName:"ol"},'"TAS-B model prefers to retrieve documents with shorter lengths"')),(0,n.kt)("p",null,"Conclusion: Maybe not only focus on a vector-based index, use a standard index\nas base + neural re-ranking"),(0,n.kt)("p",null,"(",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2104.08663.pdf)%5Bhttps://arxiv.org/pdf/2104.08663.pdf%5D"},"https://arxiv.org/pdf/2104.08663.pdf)[https://arxiv.org/pdf/2104.08663.pdf]")),(0,n.kt)("h2",{id:"other-interesting-papers"},"Other interesting papers"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Nakano et al: WebGPT (predecessor to ChatGPT) - fine-tune GPT3 to search the\nweb for QA tasks\n(",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2112.09332.pdf)%5Bhttps://arxiv.org/pdf/2112.09332.pdf%5D"},"https://arxiv.org/pdf/2112.09332.pdf)[https://arxiv.org/pdf/2112.09332.pdf]"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Schick et al: PEER: A Collaborative Language Model\n(",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2208.11663.pdf)%5Bhttps://arxiv.org/pdf/2208.11663.pdf%5D"},"https://arxiv.org/pdf/2208.11663.pdf)[https://arxiv.org/pdf/2208.11663.pdf]"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Goyal et al. 2023: Retrieval Augmented Reinforcement Learning")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Humphreys et al. 2022: Large-Scale Retrieval for Reinforcement Learning"))))}c.isMDXComponent=!0}}]);